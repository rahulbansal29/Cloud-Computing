# Cloud-Computing

- Cloud computing is the on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining physical data centers and servers, you can access technology services, such as computing power, storage, and databases, on an as-needed basis from a cloud provider.

 ![1](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/f54598b2-1241-49f4-ad0f-ca1a6e2069c2)

 
## Advantages Of Cloud Computing

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/a9991620-6886-4edd-9ed9-798efd8e6fe0)


1. Faster time to market
You can spin up new instances or retire them in seconds, allowing developers to accelerate development with quick deployments. Cloud computing supports new innovations by making it easy to test new ideas and design new applications without hardware limitations or slow procurement processes.

2. Scalability and flexibility
Cloud computing gives your business more flexibility. You can quickly scale resources and storage up to meet business demands without having to invest in physical infrastructure.
Companies don’t need to pay for or build the infrastructure needed to support their highest load levels. Likewise, they can quickly scale down if resources aren’t being used.  

3. Cost savings
Whatever cloud service model you choose, you only pay for the resources you actually use. This helps you avoid overbuilding and overprovisioning your data center and gives your IT teams back valuable time to focus on more strategic work. 

4. Better collaboration
Cloud storage enables you to make data available anywhere you are, anytime you need it. Instead of being tied to a location or specific device, people can access data from anywhere in the world from any device—as long as they have an internet connection.

5. Advanced security
Despite popular perceptions, cloud computing can actually strengthen your security posture because of the depth and breadth of security features, automatic maintenance, and centralized management.
Reputable cloud providers also hire top security experts and employ the most advanced solutions, providing more robust protection. 

6. Data loss prevention
Cloud providers offer backup and disaster recovery features. Storing data in the cloud rather than locally can help prevent data loss in the event of an emergency, such as hardware malfunction, malicious threats, or even simple user error. 



## Disadvantages Of Cloud

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/5ed08a63-48fa-4346-a1ab-da6abfa88eb7)


1. Downtime
Businesses receive cloud computing services only through the Internet. When there is an internet outage or weak connectivity, services get interrupted and this increases downtime. Therefore, one of the major criticisms of cloud computing is its high dependency on the Internet.

2. Security and Privacy
Data security and privacy threats are other disadvantages of cloud computing. According to a survey, nearly 98% of companies using cloud computing services experienced at least one data breach from 2020 to 2022. Inadequate cloud security measures lead to data leakage over cloud networks which can result in intellectual property theft, contract breaches, and malware attacks. Hackers can also control how companies provide services to their customers or end-users. This leads to a loss of business opportunities and a decrease in revenue.

3. Vulnerability to Attacks
Private clouds are considered the most secure for businesses in terms of data security. However, the cost of setting up private clouds is higher in comparison to public, hybrid, and multi-clouds. Therefore, many businesses prefer public, hybrid, and multi-cloud computing services. As these clouds provide services to multiple users over the same network, businesses become vulnerable to cyber attacks which can lead to data loss or data leakage. 

4. Limited Control and Flexibility
In public, hybrid, and community clouds, all cloud computing services are completely managed by cloud service providers. This offers limited control and flexibility to customers, restricting their access to various services and applications. Therefore, many companies enter into a separate end-user license agreements to gain control of the cloud’s services and applications.

5. Vendor Lock-in
Vendor lock-in refers to a situation where companies using cloud computing services of a particular vendor are unable to switch to a different vendor. This usually happens because of high switching costs, large amounts of data which is difficult to migrate, and several other complexities. In case of vendor lock-in, companies are forced to receive services from a particular vendor. This affects their operational workflow and efficiency.

6. Cost Concerns
Costs are both a significant advantage and disadvantage of cloud computing. While it helps small businesses avail quality services without investing large amounts to set up IT infrastructure, it can also increase expenditure for companies as there are several hidden costs involved which emerge at a later stage. These include data transfer, cloud utilization, and data migration costs.

## Gartner's Magic Quadrant

Gartner Magic Quadrant reports, the Enterprise Backup and Recovery version is a graphical presentation of a company’s Completeness of Vision and Ability to Execute their vision compared to market standards and other technologies in a defined market.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/7b4db832-8da9-4857-ab71-95a9d3c71d36)

## Public Cloud VS Private Cloud

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/84853aad-eb52-446d-992e-37ce89b82056) 

## Hybrid Cloud

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/66f2674e-9ced-4163-b55b-5d7e81406dfd)

## Cloud Service Models
1. IaaS( Infrastructure as a Service)

- IaaS is also known as Hardware as a Service (HaaS). It is a computing infrastructure managed over the internet. The main advantage of using IaaS is that it helps users to avoid the cost and complexity of purchasing and managing the physical servers.

Characteristics of IaaS

- Resources are available as a service

- Services are highly scalable

- Dynamic and flexible

- GUI and API-based access

- Automated administrative tasks

- Example: DigitalOcean, Linode, Amazon Web Services (AWS), Microsoft Azure, Google Compute Engine (GCE), Rackspace, and Cisco Metacloud.


2. PaaS (Platform as a Service) 
 - PaaS cloud computing platform is created for the programmer to develop, test, run, and manage the applications.

Characteristics of PaaS

- Accessible to various users via the same development application.

- Integrates with web services and databases.

- Builds on virtualization technology, so resources can easily be scaled up or down as per the organization's need.

- Support multiple languages and frameworks.

- Provides an ability to "Auto-scale".

- Example: AWS Elastic Beanstalk, Windows Azure, Heroku, Force.com, Google App Engine, Apache Stratos, Magento Commerce Cloud, and OpenShift.

3. (SaaS) Software as a Service 
    SaaS is also known as "on-demand software". It is a software in which the applications are hosted by a cloud service provider. Users can access these applications with the help of internet connection and 
    web browser.

### Characteristics of SaaS

- Managed from a central location

- Hosted on a remote server

- Accessible over the internet

- Users are not responsible for hardware and software updates. Updates are applied automatically.

- The services are purchased on the pay-as-per-use basis

- Example: BigCommerce, Google Apps, Salesforce, Dropbox, ZenDesk, Cisco WebEx, ZenDesk, Slack, and GoToMeeting.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/987aa19f-e5d8-49e5-9fd0-dddb5426c2af)

## S3

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/e8463619-954c-4377-818d-5e038673d07b)


- Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. S3 is built to store and retrieve any amount of data from anywhere. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile applications.

- Data is stored as objects within resources called “buckets”, and a single object can be up to 5 terabytes in size. Objects can be accessed through S3 Access Points or directly through the bucket hostname. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize data, and configure fine-tuned access controls to meet specific business, organizational, and compliance requirements.

## Features OF S3
Data lakes

Dynamic websites

Mobile applications

Backup and restore operations

Big data analytics

User-generated content

Storage archives

Enterprise applications

## EC2

- Amazon Elastic Compute Cloud (Amazon EC2) offers the broadest and deepest compute platform, with over 750 instances and choice of the latest processor, storage, networking, operating system, and purchase model to help you best match 
  the needs of your workload. We are the first major cloud provider that supports Intel, AMD, and Arm processors, the only cloud with on-demand EC2 Mac instances, and the only cloud with 400 Gbps Ethernet networking. We offer the best 
  price performance for machine learning training, as well as the lowest cost per inference instances in the cloud. More SAP, high performance computing (HPC), ML, and Windows workloads run on AWS than any other cloud.

### Use Cases

1. Run cloud-native and enterprise applications

  Amazon EC2 delivers secure, reliable, high-performance, and cost-effective compute infrastructure to meet demanding business needs.


2. Scale for HPC applications
   
  Access the on-demand infrastructure and capacity you need to run HPC applications faster and cost-effectively.



3. Develop for Apple platforms
 
  Build, test, and sign on-demand macOS workloads. Access environments in minutes, dynamically scale capacity as needed, and benefit from AWS’s pay-as-you-go pricing.



4. Train and deploy ML applications

  Amazon EC2 delivers the broadest choice of compute, networking (up to 400 Gbps), and storage services purpose-built to optimize price performance for ML projects.

  ## ELB(Elastic Load Balancer) 
  
 - Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health 
  of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic.

  ## Types Of Load Balancer

  1. Classic Load Balancer- layer 4&7
  2. Application Load Balancer- layer 7
  3. Network Load Balancer- layer 4

## Autoscaling

- AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple 
 resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB 
 tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already using Amazon EC2 Auto Scaling to 
 dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right 
 time.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/4b120d72-057c-43c5-875d-14a74f3f576d)

 
# Route 53

- Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.

 If you choose to use Route 53 for all three functions, be sure to follow the order below:

1. Register domain names
Your website needs a name, such as example.com. Route 53 lets you register a name for your website or web application, known as a domain name.

For an overview, see How domain registration works.

For a procedure, see Registering a new domain.

For a tutorial that takes you through registering a domain and creating a simple website in an Amazon S3 bucket, see Getting started with Amazon Route 53.

2. Route internet traffic to the resources for your domain
When a user opens a web browser and enters your domain name (example.com) or subdomain name (acme.example.com) in the address bar, Route 53 helps connect the browser with your website or web application.

For an overview, see How internet traffic is routed to your website or web application.

For procedures, see Configuring Amazon Route 53 as your DNS service.

For a procedure on how to route email to Amazon WorkMail, see Routing traffic to Amazon WorkMail.

3. Check the health of your resources
Route 53 sends automated requests over the internet to a resource, such as a web server, to verify that it's reachable, available, and functional. You also can choose to receive notifications when a resource becomes unavailable and choose to route internet traffic away from unhealthy resources.

For an overview, see How Amazon Route 53 checks the health of your resources.

For procedures, see Creating Amazon Route 53 health checks and configuring DNS failover.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/d59ac9c6-b4ec-43c2-b27f-820fb994300a)

# Cloud Formation

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/0447387f-82c4-4379-950d-47d1cb45b7c8)


AWS CloudFormation is an AWS service that uses template files to automate the setup of AWS resources.

It can also be described as infrastructure automation or Infrastructure-as-Code (IaC) tool and a cloud automation solution because it can automate the setup and deployment of various Infrastructure-as-a-Service (IaaS) offerings on the AWS CloudFormation supports virtually every service that runs in AWS. (A full list of supported services is available here.)

You can use CloudFormation to automate the configuration of workloads that run on the most popular AWS services, like the EC2 compute service, the S3 storage service, and the IAM service for configuring access control.

You can also apply CloudFormation templates to AWS services that cater to niche use cases, like Ground Station, the AWS satellite management solution.

In general, if a service runs on AWS, it is a safe bet that you can use CloudFormation to automate its configuration and deployment.

It is worth noting that CloudFormation is not the only way to configure and deploy services on AWS. You can handle these processes manually using the AWS command-line interface, API, or Web console. Manual provisioning is the approach that teams typically take when they are just getting started with AWS and learning how to deploy services. However, as they scale their environments up in size, many teams quickly realize that they need a solution like CloudFormation to make the deployment process faster and more consistent.

# Benefits Of Cloud formation

CloudFormation and other AWS-compatible IaC tools offer a range of benefits that make cloud service deployment and management faster and more efficient.

Deployment speed
When you create CloudFormation templates to manage how AWS resources are configured and deployed, you can deploy multiple instances of the same resources almost instantaneously using just one template. This approach leads to much faster deployment than you could achieve if you had to manually set up each deployment by running commands on the CLI or pressing buttons in the AWS console.

The caveat, of course, is that you have to spend time setting up your CloudFormation templates. However, if you will be repeating the same type of deployment several times, it will be much faster overall to create a CloudFormation template that you can reuse for each deployment than to configure each one manually.

Scaling up
Even if you do not initially expect to deploy multiple instances of the same AWS resources, CloudFormation templates are useful because they ensure that you can scale your environment up quickly when the time comes. By keeping CloudFormation templates on hand, you will know that you can add more virtual machine instances or storage space, for example, at a moment's notice if your applications experience increased traffic and you need to scale your environment up.

Alternatively, when demand decreases and you want to scale down to save money, you can take some of your deployments offline while still retaining the ability to redeploy them quickly using CloudFormation when demand increases.

Service integration
A single CloudFormation template can manage the deployment of individual services or resources and multiple resources. This management ability means you can use CloudFormation to integrate different AWS cloud services. For example, you could write a template that sets up an EC2 virtual machine within an AWS Virtual Private Cloud (VPC) or deploys an S3 storage bucket and configures access control for it using the IAM service.

Managing multiple services through a single template makes it easy to integrate AWS services as you build out a complete cloud environment.

Consistency
When you use CloudFormation templates to define and deploy AWS resources, you can apply precisely the same configuration repeatedly. In this way, CloudFormation ensures that your applications and services will be consistent and identical, no matter how many instances you create.

The alternative approach, which is to set up each resource by hand, introduces the risk that the engineer who performs the work might apply different settings to different instances, resulting in inconsistency. In turn, your environment would be more challenging to manage because some resources would look different than others, even if they perform the same primary job. You might have different types of EC2 instances hosting replicas of the same application, for instance, or different IAM access controls for the same service. This inconsistency would make it challenging to manage resources uniformly.

Security
Along similar lines, although CloudFormation is not a security tool per se, it can improve the overall security of your AWS environment by reducing the risk of oversights or human errors that could turn into breaches. As long as you design your CloudFormation templates to be secure, you do not need to worry that an engineer who deploys resources will forget to turn on important access control, for example, or leave data exposed to unrestricted, public access.

Easy updates
In addition to deploying new resources, you can apply changes to existing resources with CloudFormation templates. This ability simplifies the process of, for example, adding more storage to a fleet of ec2 instances or changing access control rules.

Auditing and change management
When you use CloudFormation to manage your infrastructure, you can track changes based on which templates you have applied and how they change over time. Change tracking in CloudFormation means that you will be able to determine how your AWS services and resources have changed over time without looking through logs to reconstruct the timeline of updates.

# Cloud Frnont

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/6ae44b53-5e80-4cac-904e-4695b213496f)


Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations.

When a user requests content that you’re serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.

If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.
If the content is not in that edge location, CloudFront retrieves it from an origin that you’ve defined — such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.
As an example, suppose that you’re serving an image from a traditional web server, not from CloudFront. For example, you might serve an image, sunsetphoto.png, using the URL http://example.com/sunsetphoto.png.

Your users can easily navigate to this URL and see the image. But they probably don’t know that their request is routed from one network to another — through the complex collection of interconnected networks that comprise the internet — until the image is found.

CloudFront speeds up the distribution of your content by routing each user request through the AWS backbone network to the edge location that can best serve your content. Typically, this is a CloudFront edge server that provides the fastest delivery to the viewer.

Using the AWS network dramatically reduces the number of networks that your users’ requests must pass through, which improves performance. Users get lower latency — the time it takes to load the first byte of the file — and higher data transfer rates.

You also get increased reliability and availability because copies of your files (also known as objects) are now held (or cached) in multiple edge locations around the world.

How Does Amazon CloudFront Work?
CloudFront works seamlessly with any AWS origin, such as Amazon S3, Amazon EC2, Elastic Load Balancing, or with any custom HTTP origin. You can customize your content delivery through CloudFront using the secure and programmable edge computing features CloudFront Functions and AWS Lambda@Edge.

Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations.

When a user requests content that you’re serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.

If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.
If the content is not in that edge location, CloudFront retrieves it from an origin that you’ve defined — such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.
As an example, suppose that you’re serving an image from a traditional web server, not from CloudFront. For example, you might serve an image, sunsetphoto.png, using the URL http://example.com/sunsetphoto.png.

Your users can easily navigate to this URL and see the image. But they probably don’t know that their request is routed from one network to another — through the complex collection of interconnected networks that comprise the internet — until the image is found.

CloudFront speeds up the distribution of your content by routing each user request through the AWS backbone network to the edge location that can best serve your content. Typically, this is a CloudFront edge server that provides the fastest delivery to the viewer.

Using the AWS network dramatically reduces the number of networks that your users’ requests must pass through, which improves performance. Users get lower latency — the time it takes to load the first byte of the file — and higher data transfer rates.

You also get increased reliability and availability because copies of your files (also known as objects) are now held (or cached) in multiple edge locations around the world.

How Does Amazon CloudFront Work?
CloudFront works seamlessly with any AWS origin, such as Amazon S3, Amazon EC2, Elastic Load Balancing, or with any custom HTTP origin. You can customize your content delivery through CloudFront using the secure and programmable edge computing features CloudFront Functions and AWS Lambda@Edge.

# Disaster Recovery

Disaster recovery strategies available to you within AWS can be broadly categorized into four approaches, ranging from the low cost and low complexity of making backups to more complex strategies using multiple active Regions. Active/passive strategies use an active site (such as an AWS Region) to host the workload and serve traffic. The passive site (such as a different AWS Region) is used for recovery. The passive site does not actively serve traffic until a failover event is triggered.

It is critical to regularly assess and test your disaster recovery strategy so that you have confidence in invoking it, should it become necessary. Use AWS Resilience Hub to continuously validate and track the resilience of your AWS workloads, including whether you are likely to meet your RTO and RPO targets.


      Graph showing disaster recovery strategies and highlights of each strategy.
    
![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/bb83a069-0b27-4b12-bbe9-756c12a8fb33)


For a disaster event based on disruption or loss of one physical data center for a well-architected, highly available workload, you may only require a backup and restore approach to disaster recovery. If your definition of a disaster goes beyond the disruption or loss of a physical data center to that of a Region or if you are subject to regulatory requirements that require it, then you should consider Pilot Light, Warm Standby, or Multi-Site Active/Active.

When choosing your strategy, and the AWS resources to implement it, keep in mind that within AWS, we commonly divide services into the data plane and the control plane. The data plane is responsible for delivering real-time service while control planes are used to configure the environment. For maximum resiliency, you should use only data plane operations as part of your failover operation. This is because the data planes typically have higher availability design goals than the control planes.

Backup and restore

Backup and restore is a suitable approach for mitigating against data loss or corruption. This approach can also be used to mitigate against a regional disaster by replicating data to other AWS Regions, or to mitigate lack of redundancy for workloads deployed to a single Availability Zone. In addition to data, you must redeploy the infrastructure, configuration, and application code in the recovery Region. To enable infrastructure to be redeployed quickly without errors, you should always deploy using infrastructure as code (IaC) using services such as AWS CloudFormation or the AWS Cloud Development Kit (AWS CDK). Without IaC, it may be complex to restore workloads in the recovery Region, which will lead to increased recovery times and possibly exceed your RTO. In addition to user data, be sure to also back up code and configuration, including Amazon Machine Images (AMIs) you use to create Amazon EC2 instances. You can use AWS CodePipeline to automate redeployment of application code and configuration.

# Docker Containers

- Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, 
 system tools, code, and runtime. Using Docker, you can quickly deploy and scale applications into any environment and know your code will run.
 Running Docker on AWS provides developers and admins a highly reliable, low-cost way to build, ship, and run distributed applications at any scale.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/8fbbe94d-c647-40a5-a22a-d002c66cd06f)


## How Docker Container Work

Docker works by providing a standard way to run your code. Docker is an operating system for containers. Similar to how a virtual machine virtualizes (removes the need to directly manage) server hardware, containers virtualize the operating system of a server. Docker is installed on each server and provides simple commands you can use to build, start, or stop containers.

AWS services such as AWS Fargate, Amazon ECS, Amazon EKS, and AWS Batch make it easy to run and manage Docker containers at scale.

## Docker VS Virtual Machine

Following are the significant differences between Docker and virtual machines.

## OS Support and ArchitectureThe main difference between Docker and VMs lies in their architecture, demonstrated below.

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/13a34116-5340-4c12-ba98-bd7d02bf38b7)

Docker vs. Virtual Machines VMs have the host OS and guest OS inside each VM. A guest OS can be any OS, like Linux or Windows, irrespective of the host OS. In contrast, Docker containers host on a single physical server with a host OS, which shares among them. Sharing the host OS between containers makes them light and increases the boot time. Docker containers are considered suitable to run multiple applications over a single OS kernel; whereas, virtual machines are needed if the applications or services required to run on different OS. 

## Security

The second difference between VMs and Docker is that Virtual Machines are stand-alone with their kernel and security features. Therefore, applications needing more privileges and security run on virtual machines. On the flip side, providing root access to applications and running them with administrative premises is not recommended in the case of Docker containers because containers share the host kernel. The container technology has access to the kernel subsystems; as a result, a single infected application is capable of hacking the entire host system.

## Portability

Another relevant Docker vs Virtual Machine difference is about portability: VMs are isolated from their OS, and so they are not ported across multiple platforms without incurring compatibility issues. At the development level, if an application is to be tested on different platforms, then Docker containers must be considered. Docker container packages are self-contained and can run applications in any environment, and since they don’t need a guest OS, they can be easily ported across different platforms. Docker containers can be easily deployed in servers since containers being lightweight can be started and stopped in very less time compared to virtual machines.

## Performance

The last main Docker vs VM difference refers to performance: Virtual Machines are more resource-intensive than Docker containers as the virtual machines need to load the entire OS to start. The lightweight architecture of Docker containers is less resource-intensive than virtual machines. In the case of a virtual machine, resources like CPU, memory, and I/O may not be allocated permanently to containers — unlike in the case of a Docker container, where the resource usage works with the load or traffic. Scaling up and duplicating a Docker container is simple and easy as compared to a virtual machine because there is no need to install an operating system in them.

## Difference Between Docker And VM

![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/997aa1f3-cd43-4d4d-8a90-3b6a736d0f7f)

# AWS ELASTIC BEANSTALK

- Elastic Beanstalk is a platform within AWS that is used for deploying and scaling web applications. In simple terms this platform as a service (PaaS) takes your application code and deploys it while provisioning the supporting architecture and compute resources required for your code to run. Elastic Beanstalk also fully manages the patching and security updates for those provisioned resources. 

- There are many PaaS solutions in the cloud computing space including Redhat Open Shift, Google App Engine, Scalingo, Python Anywhere, Azure App Service, however AWS Elastic Beanstalk remains one of the leading PaaS choices among app developers.

- There is no charge to use Elastic Beanstalk to deploy your applications, you are only charged for the resources that are created to support your application.

- If you are planning to deploy Elastic Beanstalk, you can use Hava to visualise your architecture.

- Amazon Web Services provides a cloud deployment service named Elastic Beanstalk. With the help of this service, you can easily set the deployment of your application to the AWS cloud; you just need to upload the files, and it’s 
 done. There are some useful operations that AWS Elastic Beanstalk provides that include provisioning, load balancing, health monitoring of the deployed application, and autoscaling of the application. The applications that are 
 developed with Python, Java, PHP, Node.js, and Docker can easily be deployed on commonly used servers such as Apache, Nginx, and Passenger. The code that you have created in any language from the list above can easily be uploaded on 
 the AWS Elastic Beanstalk service so that it will handle all the operations itself. However, to use Amazon Web Services, you need to pay money, but it doesn’t mean that you need to pay an extra charge for using Elastic Beanstalk. 

- There are no additional charges for using AWS Elastic Beanstalk, and you only need to pay for AWS resources that are needed to store and run the application that you created. 

- To better understand Elastic Beanstalk, you need to know EC2, which is an acronym used for Amazon Compute Cloud. EC2 is just like a virtual machine where you can use the same instances as a virtual machine. 

- AWS Elastic Beanstalk is easy to use as it reduces the complexity without limiting its features or the control over its services. 



  ![image](https://github.com/rahulbansal29/Cloud-Computing/assets/145260475/da8e8d0e-68c5-49df-9ad7-d263266437b0)




























